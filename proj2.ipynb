{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skrytp realizujacy projekt drugi z przedmiotu Podstawy Sztucznej Inteligencji\n",
    "Temat projektu(TM.K3): Utworzenie oraz porownanie w dzialaniu 2 klasyfikatorow: wielowarstwowej sieci neuronowej(wybrany został perceptron dwuwarstwowy) oraz sieci konwolucyjnej. Przy tworzeniu i uczeniu modelu wykorzystano walidacje k-krotna.\n",
    "Zespol:\n",
    "Mateusz Grochowina\n",
    "Jan Zgorzelski\n",
    "Rozklad zadan:\n",
    "Opracowanie i tesotwanie sieci wielowarstwowej - Jan\n",
    "Opracowanie i testowanie sieci konwolucyjnej - Mateusz\n",
    "Opracowanie dokumentacji oraz prezentacji wynikow - praca wspolna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcja jako argumenty przyjmuje aktualne zbiory danych uczacy i walidacyjny, zwraca wektor wartosci bledow i dopasowania dla zbiorow walidacyjnego i uczacego.\n",
    "\n",
    "Decyzje projektowe:\n",
    "1. Sieć składa się z podanych głównych warstw(po kolei): konwolucyjna,poolingu,konwulucyjna,poolingu,dropout-u.\n",
    "     Podana architektura zostala dobrana poprzez eksperymenty z rozna iloscia i rodzajem warstw. Testowane byly miedzy innymi sieci z dodatkowymą jedną i dwoma parami konwolucyjna-pooling, wyniki dla nich nie były jednak zadowalajace. Wyniki roznily sie bardzo niewiele(3 miejsce po przecinku) ale czasowo wykonywaly sie bardzo dlugo. Warstwa dropoutu zostala dodana na sam koniec a nie dla przykladu po pierwszej warstwie poolingu, poniewaz chciano odrzucic w uczeniu neurony po przejsciu przez pozostale warstwy, aby nie zaklocic uczenia pozostalych. Parametr dropoutu ustawiono na 0.2, nie eksperymentowano z doborem wartosci tego parametru, ustawiona odgornie.\n",
    "     \n",
    "2. Jako funkcje aktywacji uzyto \"ReLU\". Spowodowane to bylo tym, ze podana funkcja zmniejsza przeuczenie.\n",
    "\n",
    "3. Liczba epok ustalono na 3, dla testow dla wyzszej ilosci np. 5 roznica czwartego miejsca po przecinku dla 10 epok roznica o 0.001. Jednak kryterium czasowe zdecydowalo o wyborze 3 epok.\n",
    "\n",
    "4. Funkcja aktywacji ostatniej warstwy zostala ustawiona jako \"softmax\". Dobiera ona klasyfikacje dla poszczegolnych obietkow jako analize prawdopodobienstw przynaleznosci danego obiektu do danej klasy.\n",
    "\n",
    "5. Dane do wynikow i wnioskow z procesu uczenia pobrano z funkcji uczacej za pomoca obiektu \"History\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funkcja wyznaczajaca model za pomoca sieci konwolucyjnej   \n",
    "def cnn(X_train,X_val,y_train,y_val):\n",
    "    \n",
    "    data = []\n",
    "    #Dobranie rozmiaru obrazka\n",
    "    X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "    X_val = X_val.reshape(X_val.shape[0], 28, 28, 1)\n",
    "\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_val = X_val.astype('float32')\n",
    "\n",
    "    #normalizacja\n",
    "    X_train/=255\n",
    "    X_val/=255\n",
    "\n",
    "    #liczba klas(cyfr)\n",
    "    n_classes = 10\n",
    "\n",
    "    #kodowanie etykiet\n",
    "    Y_train = np_utils.to_categorical(y_train, n_classes)\n",
    "    Y_val = np_utils.to_categorical(y_val, n_classes)\n",
    "\n",
    "    #tworzenie modelu sieci\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(28,28,1)))#pierwsza warstwa konwolucyjna\n",
    "    model.add(Activation('relu'))#funkcja aktywacji\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))#pierwsza warstwa poolingu\n",
    "    model.add(Conv2D(32, (3, 3)))#druga warstwa konwolucyjna\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))#druga warstwa poolingu\n",
    "\n",
    "    model.add(Flatten())#splaszczenie wejsc\n",
    "\n",
    "    #Polaczenie warstw\n",
    "    model.add(Dense(512))#aktywacja wyjsc pierwszych warstw\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))#\"wygaszenie\" czesci polaczen\n",
    "    model.add(Dense(10))#warstwa klas\n",
    "\n",
    "    #funkcja zliczajaca prawdopodobienstwa\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    #kompilacja modelu\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "    #rozszerzanie danych\n",
    "    gen = ImageDataGenerator(rotation_range=8, width_shift_range=0.08, shear_range=0.3,\n",
    "                             height_shift_range=0.08, zoom_range=0.08)\n",
    "\n",
    "    test_gen = ImageDataGenerator()\n",
    "\n",
    "    #tworzenie partii danych\n",
    "    train_gener = gen.flow(X_train, Y_train, batch_size=64)\n",
    "    val_gener = test_gen.flow(X_val, Y_val, batch_size=64)\n",
    "\n",
    "    #zmienne pomocnicze\n",
    "    size_train = X_train.shape[0]\n",
    "    size_test = X_val.shape[0]\n",
    "\n",
    "    #uczenie sieci\n",
    "    history = model.fit_generator(train_gener, steps_per_epoch=size_test//64, epochs=3, \n",
    "                        validation_data=val_gener, validation_steps=size_test//64)\n",
    "\n",
    "    #wyniki    \n",
    "\n",
    "    a = history.history['acc']\n",
    "    b = history.history['loss']\n",
    "    c = history.history['val_acc']\n",
    "    d = history.history['val_loss']\n",
    "    \n",
    "    data.append(a[2])\n",
    "    data.append(b[2])\n",
    "    data.append(c[2])\n",
    "    data.append(d[2])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siec neuronowa - perceptron wielowarstwowy.\n",
    "\n",
    "Funkcja jako argumenty przyjmuje aktualne zbiory danych uczacy i walidacyjny, zwraca wektor wartosci bledow i dopasowania dla zbiorow walidacyjnego i uczacego.\n",
    "\n",
    "Dobrano hyperparametry sieci\n",
    "\n",
    "Liczba warstw ukrytych: 2\n",
    "Liczba neuronów: 500\n",
    "Liczba epok: 3\n",
    "Batch_size: 32\n",
    "\n",
    "Dobór hyperparametrów sieci:\n",
    "\n",
    "1. Liczba warstw ukrytych sieci, analizie poddane zostały wartwy ukryte aktywowane funkcją ReLu - Rectified Linear Units jest to przyblizenie funkcji Softplus przez proste progowanie w zerze. Zabieg ten przyspiesza zarówno implementacje jak i obliczenia algorytmu w porównaniu do funkcji sigmoidalnej i tanh. \n",
    "\n",
    "2. Warstwa wyjsciowa: 10 jednostek, poniewaz 10 klas. Klasy wykluczaja sie wzajemnie wiec zastosowano funkcje softmax lub inaczej znormalizowana funkcje wykładnicza, jest uogólnieniem funkcji sigmoidalnej\n",
    "\n",
    "3. Liczba jednostek wartw ukrytych, jest to liczba neuronow, badane zostaly wartosci od 50 - 2000, dla eksperymentu obejmujacego 3 warstwy ukryte po 2000 neuronow, ktore trenowane byly przez 5 epok wykryto przeuczenie\n",
    "4. Liczba epok - liczba epok odpowiada przejsciu przez siec w przod i do tylu wszystkich przykladow treningowych. Liczba epok zostala ustawiona na 3, dla testow dla wyzszej ilosci np. 5 roznica czwartego miejsca po przecinku dla 10 epok roznica o 0.001. Jednak kryterium czasowe zdecydowalo o wyborze 3 epok.\n",
    "\n",
    "5. Batch_size - wielkosc partii czyli liczba przykladow treningowych, eksperymentalne badanie wartosci tego parametru doprowadzilo do ostateczno zostawienia defaultowej wartosci Kerasa = 32\n",
    "\n",
    "Dane do wynikow i wnioskow z procesu uczenia pobrano z funkcji uczacej za pomoca obiektu \"History\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funkcja wyznaczajaca model za perceptronu\n",
    "def perceptron(x_train,x_test,y_train,y_test):\n",
    "    \n",
    "    data = []\n",
    "    # Normalizacja danych \n",
    "    x_train = tf.keras.utils.normalize(x_train, axis=1)  \n",
    "    x_test = tf.keras.utils.normalize(x_test, axis=1)  \n",
    "\n",
    "    \n",
    "    # Budowanie modelu sekwencyjnego, z wyprzedzeniem (feed-forward)\n",
    "    model = tf.keras.models.Sequential()\n",
    "    # Przetworzenie warstwy wielowymiarowej 28x28 na plaska 1x784\n",
    "    model.add(tf.keras.layers.Flatten())  \n",
    "    # Dodanie dwóch warstw ukrytych aktywowanych funkcja ReLu, 128 jednostek\n",
    "    model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))  \n",
    "    model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))  \n",
    "    # Dodanie warstwy wyjsciowej 10 jednostek 10 klas, funkcja Softmax - rozkładu prawdopodobieństwa \n",
    "    model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))  \n",
    "\n",
    "    #Kompilacja modelu \n",
    "    model.compile(optimizer='adam',  # Wybor optymalizatora\n",
    "              loss='sparse_categorical_crossentropy',  # Sposob obliczania miary strat\n",
    "              metrics=['accuracy'])  # Co jest sledzone \n",
    "    \n",
    "    # Trenowanie modelu przez 3 epoki\n",
    "    \n",
    "    History=model.fit(x_train,y_train,epochs=3)  \n",
    "\n",
    "    \n",
    "   # Wyciagniecie danych historycznych ze struktury  \n",
    "   \n",
    "    cd=History.history\n",
    "    c =History.history['loss']\n",
    "    d=History.history['acc']\n",
    "    \n",
    "    data.append(d[2])\n",
    "    data.append(c[2])\n",
    "\n",
    "    # Przetestowanie modelu, ocena danych próbki z modelem\n",
    "    \n",
    "    val_loss, val_acc = model.evaluate(x_test, y_test)   \n",
    "    \n",
    "    #Sumowanie precyzji walidacji w wektor\n",
    "    data.append(val_acc)\n",
    "    \n",
    "    #Sumowanie błędów walidacji w wektor\n",
    "    data.append(val_loss)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcja odpowiedzialna za prezentacje danych dla danej sieci.\n",
    "Przyjmuje jako argument tablice danych z danej sieci(wartosci bledow i dopasowan dla kazdej walidacji).\n",
    "Wykonuje wykresy wartosci bledu i dopasowania w zaleznosci od numeru walidacji oraz wyswietla srednie wartosci bledow oraz dopasowania dla zbiorow uczacych i walidacyjnych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funkcja opracowujaca wyniki dzialania programu\n",
    "def create_results(data):\n",
    "\n",
    "    data0 = []\n",
    "    data1 = []\n",
    "    data2 = []\n",
    "    data3 = []\n",
    "    ret_data = []\n",
    "    for i in range(5):\n",
    "        data0.append(data[i][0])\n",
    "        data1.append(data[i][1])\n",
    "        data2.append(data[i][2])\n",
    "        data3.append(data[i][3])\n",
    "    # Wykres funkcji rozpoznan cyfr\n",
    "    plt.plot(data0,'-go')\n",
    "    plt.plot(data2,'-ro')\n",
    "    plt.title('Rozpocznania cyfr w zaleznosci od zbioru')\n",
    "    plt.ylabel('Dopasowanie')\n",
    "    plt.xlabel('Numer walidacji')\n",
    "    plt.legend(['Train', 'Val'], loc='right')\n",
    "    plt.show()\n",
    "\n",
    "    # Wykres funkcji straty\n",
    "    plt.plot(data1,'-go')\n",
    "    plt.plot(data3,'-ro')\n",
    "    plt.title('Wartosc funkcji kary w zaleznosci od zbioru')\n",
    "    plt.ylabel('Funkcja kary')\n",
    "    plt.xlabel('Numer walidacji')\n",
    "    plt.legend(['Train','Val'], loc='right')\n",
    "    plt.show()\n",
    "\n",
    "    print('Srednia wartosc funkcji bledu dla zbiorow uczacych')\n",
    "    print(np.mean(data1))\n",
    "\n",
    "    print('Srednia wartosc funkcji bledu dla zbiorow walidacyjnych')\n",
    "    print(np.mean(data3))\n",
    "\n",
    "    print('Srednia wartosc dopasowan dla zbiorow uczacych')\n",
    "    print(np.mean(data0))\n",
    "\n",
    "    print('Srednia wartosc dopasowan dla zbiorow walidacyjnych')\n",
    "    print(np.mean(data2))\n",
    "    \n",
    "    ret_data.append(np.mean(data3))\n",
    "    ret_data.append(np.mean(data2))\n",
    "    \n",
    "    return ret_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 . petla walidacji dla perceptronu\n",
      "WARNING:tensorflow:From D:\\app\\matig\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Epoch 1/3\n",
      "47995/47995 [==============================] - 5s 105us/sample - loss: 0.2902 - acc: 0.9159\n",
      "Epoch 2/3\n",
      "47995/47995 [==============================] - 4s 81us/sample - loss: 0.1195 - acc: 0.9628\n",
      "Epoch 3/3\n",
      "47995/47995 [==============================] - 4s 82us/sample - loss: 0.0814 - acc: 0.9745\n",
      "12005/12005 [==============================] - 1s 42us/sample - loss: 0.1176 - acc: 0.9645\n",
      "1 . petla walidacji dla sieci konwolucyjnej\n",
      "WARNING:tensorflow:From D:\\app\\matig\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From D:\\app\\matig\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/3\n",
      "188/188 [==============================] - 3s 14ms/step - loss: 0.0688 - acc: 0.9785\n",
      "750/750 [==============================] - 37s 50ms/step - loss: 0.2856 - acc: 0.9094 - val_loss: 0.0688 - val_acc: 0.9785\n",
      "Epoch 2/3\n",
      "188/188 [==============================] - 3s 14ms/step - loss: 0.0515 - acc: 0.9824\n",
      "750/750 [==============================] - 36s 48ms/step - loss: 0.0980 - acc: 0.9682 - val_loss: 0.0515 - val_acc: 0.9824\n",
      "Epoch 3/3\n",
      "188/188 [==============================] - 3s 14ms/step - loss: 0.0472 - acc: 0.9852\n",
      "750/750 [==============================] - 38s 50ms/step - loss: 0.0693 - acc: 0.9782 - val_loss: 0.0472 - val_acc: 0.9852\n",
      "2 . petla walidacji dla perceptronu\n",
      "Epoch 1/3\n",
      "47998/47998 [==============================] - 5s 105us/sample - loss: 0.2866 - acc: 0.9170\n",
      "Epoch 2/3\n",
      "47998/47998 [==============================] - 4s 92us/sample - loss: 0.1172 - acc: 0.9646\n",
      "Epoch 3/3\n",
      "47998/47998 [==============================] - 5s 94us/sample - loss: 0.0774 - acc: 0.9755\n",
      "12002/12002 [==============================] - 1s 43us/sample - loss: 0.1118 - acc: 0.9672\n",
      "2 . petla walidacji dla sieci konwolucyjnej\n",
      "Epoch 1/3\n",
      "188/188 [==============================] - 3s 14ms/step - loss: 0.0613 - acc: 0.9810\n",
      "750/750 [==============================] - 37s 49ms/step - loss: 0.2804 - acc: 0.9119 - val_loss: 0.0613 - val_acc: 0.9810\n",
      "Epoch 2/3\n",
      "188/188 [==============================] - 3s 14ms/step - loss: 0.0525 - acc: 0.9837\n",
      "750/750 [==============================] - 37s 49ms/step - loss: 0.0988 - acc: 0.9692 - val_loss: 0.0525 - val_acc: 0.9837\n",
      "Epoch 3/3\n",
      "188/188 [==============================] - 3s 14ms/step - loss: 0.0387 - acc: 0.9877\n",
      "750/750 [==============================] - 37s 50ms/step - loss: 0.0716 - acc: 0.9776 - val_loss: 0.0387 - val_acc: 0.9877\n",
      "3 . petla walidacji dla perceptronu\n",
      "Epoch 1/3\n",
      "48000/48000 [==============================] - 5s 96us/sample - loss: 0.2867 - acc: 0.9166\n",
      "Epoch 2/3\n",
      "48000/48000 [==============================] - 4s 87us/sample - loss: 0.1153 - acc: 0.9642\n",
      "Epoch 3/3\n",
      "48000/48000 [==============================] - 4s 87us/sample - loss: 0.0766 - acc: 0.9758\n",
      "12000/12000 [==============================] - 1s 48us/sample - loss: 0.1058 - acc: 0.9682\n",
      "3 . petla walidacji dla sieci konwolucyjnej\n",
      "Epoch 1/3\n",
      "188/188 [==============================] - 3s 15ms/step - loss: 0.0670 - acc: 0.9783\n",
      "750/750 [==============================] - 38s 50ms/step - loss: 0.2991 - acc: 0.9082 - val_loss: 0.0670 - val_acc: 0.9783\n",
      "Epoch 2/3\n",
      "188/188 [==============================] - 3s 15ms/step - loss: 0.0558 - acc: 0.9831\n",
      "750/750 [==============================] - 38s 50ms/step - loss: 0.1023 - acc: 0.9687 - val_loss: 0.0558 - val_acc: 0.9831\n",
      "Epoch 3/3\n",
      "188/188 [==============================] - 3s 15ms/step - loss: 0.0420 - acc: 0.9867\n",
      "750/750 [==============================] - 39s 52ms/step - loss: 0.0739 - acc: 0.9771 - val_loss: 0.0420 - val_acc: 0.9867\n",
      "4 . petla walidacji dla perceptronu\n",
      "Epoch 1/3\n",
      "48003/48003 [==============================] - 5s 103us/sample - loss: 0.2888 - acc: 0.9169\n",
      "Epoch 2/3\n",
      "48003/48003 [==============================] - 5s 96us/sample - loss: 0.1183 - acc: 0.9638\n",
      "Epoch 3/3\n",
      "48003/48003 [==============================] - 5s 98us/sample - loss: 0.0797 - acc: 0.9758\n",
      "11997/11997 [==============================] - 1s 51us/sample - loss: 0.1167 - acc: 0.9651\n",
      "4 . petla walidacji dla sieci konwolucyjnej\n",
      "Epoch 1/3\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 0.0668 - acc: 0.9792\n",
      "751/751 [==============================] - 41s 54ms/step - loss: 0.3118 - acc: 0.9030 - val_loss: 0.0668 - val_acc: 0.9792\n",
      "Epoch 2/3\n",
      "188/188 [==============================] - 3s 15ms/step - loss: 0.0404 - acc: 0.9869\n",
      "751/751 [==============================] - 40s 53ms/step - loss: 0.0986 - acc: 0.9690 - val_loss: 0.0404 - val_acc: 0.9869\n",
      "Epoch 3/3\n",
      "188/188 [==============================] - 3s 15ms/step - loss: 0.0369 - acc: 0.9887\n",
      "751/751 [==============================] - 39s 52ms/step - loss: 0.0731 - acc: 0.9776 - val_loss: 0.0369 - val_acc: 0.9887\n",
      "5 . petla walidacji dla perceptronu\n",
      "Epoch 1/3\n",
      "48004/48004 [==============================] - 5s 105us/sample - loss: 0.2933 - acc: 0.9153\n",
      "Epoch 2/3\n",
      "48004/48004 [==============================] - 5s 101us/sample - loss: 0.1194 - acc: 0.9634\n",
      "Epoch 3/3\n",
      "48004/48004 [==============================] - 5s 102us/sample - loss: 0.0781 - acc: 0.9758\n",
      "11996/11996 [==============================] - 1s 57us/sample - loss: 0.0924 - acc: 0.9708\n",
      "5 . petla walidacji dla sieci konwolucyjnej\n",
      "Epoch 1/3\n",
      "188/188 [==============================] - 3s 15ms/step - loss: 0.0578 - acc: 0.9834\n",
      "751/751 [==============================] - 37s 50ms/step - loss: 0.2928 - acc: 0.9077 - val_loss: 0.0578 - val_acc: 0.9834\n",
      "Epoch 2/3\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 0.0384 - acc: 0.9882\n",
      "751/751 [==============================] - 38s 50ms/step - loss: 0.0997 - acc: 0.9696 - val_loss: 0.0384 - val_acc: 0.9882\n",
      "Epoch 3/3\n",
      "188/188 [==============================] - 3s 14ms/step - loss: 0.0354 - acc: 0.9882\n",
      "751/751 [==============================] - 39s 52ms/step - loss: 0.0722 - acc: 0.9774 - val_loss: 0.0354 - val_acc: 0.9882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Srednia wartosc funkcji bledu dla zbiorow uczacych\n",
      "0.07861859006825375\n",
      "Srednia wartosc funkcji bledu dla zbiorow walidacyjnych\n",
      "0.10886761971018614\n",
      "Srednia wartosc dopasowan dla zbiorow uczacych\n",
      "0.9754833\n",
      "Srednia wartosc dopasowan dla zbiorow walidacyjnych\n",
      "0.967167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Srednia wartosc funkcji bledu dla zbiorow uczacych\n",
      "0.07204794954091705\n",
      "Srednia wartosc funkcji bledu dla zbiorow walidacyjnych\n",
      "0.04005379680911961\n",
      "Srednia wartosc dopasowan dla zbiorow uczacych\n",
      "0.9775959\n",
      "Srednia wartosc dopasowan dla zbiorow walidacyjnych\n",
      "0.9872836\n",
      "Sredni blad dla perceptronu dla zbiorow uczacych: 0.10886761971018614\n",
      "Sredni blad dla sieci konwolucyjnej dla zbiorow uczacych: 0.04005379680911961\n",
      "Rozpoznanie dla perceptronu dla zbiorow walidacyjnych: 0.967167\n",
      "Rozpoznanie dla sieci konwolucyjnej dla zbiorow walidacyjnych: 0.9872836\n"
     ]
    }
   ],
   "source": [
    "#skrypt realizujacy glowna funkcje programu\n",
    "\n",
    "#biblioteki\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import statistics\n",
    "\n",
    "perc_data1 = []\n",
    "cnn_data1 = []\n",
    "\n",
    "#pobranie zbiorow uczacych i testowych\n",
    "(X, y),(x_test, y_test) = mnist.load_data()\n",
    "\n",
    "i = 1\n",
    "\n",
    "#walidacja k-krotna\n",
    "skf = StratifiedKFold(n_splits=5,shuffle = True)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_val = X[train_index], X[test_index]\n",
    "    y_train, y_val = y[train_index], y[test_index]\n",
    "    print(i,'. petla walidacji dla perceptronu')\n",
    "    perc_data = perceptron(X_train,X_val,y_train,y_val)\n",
    "    perc_data1.append(perc_data)\n",
    "    print(i,'. petla walidacji dla sieci konwolucyjnej')\n",
    "    cnn_data = cnn(X_train,X_val,y_train,y_val)\n",
    "    cnn_data1.append(cnn_data)\n",
    "    i = i+1\n",
    "    \n",
    "#wyniki dla perceptronu\n",
    "res_perc = create_results(perc_data1)\n",
    "\n",
    "#wyniki dla sieci konwolucyjnej\n",
    "res_cnn = create_results(cnn_data1)\n",
    "\n",
    "#porownanie obu sieci\n",
    "print('Sredni blad dla perceptronu dla zbiorow uczacych:',res_perc[0])\n",
    "print('Sredni blad dla sieci konwolucyjnej dla zbiorow uczacych:',res_cnn[0])\n",
    "\n",
    "print('Rozpoznanie dla perceptronu dla zbiorow walidacyjnych:',res_perc[1])\n",
    "print('Rozpoznanie dla sieci konwolucyjnej dla zbiorow walidacyjnych:',res_cnn[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
